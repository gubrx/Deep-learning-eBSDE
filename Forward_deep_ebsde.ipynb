{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gubrx/Deep-learning-eBSDE/blob/main/Forward_deep_ebsde.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMsnWDcC_AZ0"
      },
      "source": [
        "# Drive_import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MZqJ8pDG_E_k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from scipy.integrate import quad\n",
        "import scipy\n",
        "import scipy.optimize as opt\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EdTQxWh7AWv"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ny5bK7E07F6Z"
      },
      "outputs": [],
      "source": [
        "#Neural Network\n",
        "class Net( tf.keras.Model):\n",
        "    def __init__(self, cond_lambd, lambda_lim, dim, nbNeurons, activation= tf.nn.tanh):\n",
        "        super().__init__()\n",
        "        self.nbNeurons = nbNeurons\n",
        "        self.dim = dim\n",
        "        self.ListOfDense = [layers.Dense( nbNeurons[i],activation= activation, kernel_initializer= tf.keras.initializers.GlorotNormal())  for i in range(len(nbNeurons)) ] +[layers.Dense(self.dim, activation= None, kernel_initializer= tf.keras.initializers.GlorotNormal())]\n",
        "        if cond_lambd:\n",
        "          self.lambd= tf.Variable(tf.keras.initializers.GlorotNormal()([1]),  trainable = True, dtype=tf.float32, constraint=lambda t: tf.clip_by_value(t, -lambda_lim, lambda_lim))\n",
        "\n",
        "    def call(self,inputs):\n",
        "        x = inputs\n",
        "        for layer in self.ListOfDense:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00eIlwGw7G6J"
      },
      "source": [
        "# StochasticFactor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MMEgFE7x3ZBX"
      },
      "outputs": [],
      "source": [
        "class StochasticFactor():\n",
        "    def __init__(self, x0, dt, T_H, dim):\n",
        "        self.x0 = x0\n",
        "        self.dt = dt\n",
        "        self.T_H = T_H\n",
        "        self.dim = dim\n",
        "\n",
        "    def one_step(self, V, mu, kappa):\n",
        "        dw_sample = np.sqrt(self.dt)*np.random.normal(size=(len(V), self.dim))\n",
        "        V = V + mu(V)*self.dt + np.sum(np.array(kappa)*dw_sample, axis=1)\n",
        "        return dw_sample, V\n",
        "\n",
        "    def sample(self, mu, kappa, num_sample):\n",
        "        V = np.ones((1, num_sample)) * self.x0\n",
        "        dW = np.empty((0, num_sample, self.dim))\n",
        "        T_A = np.zeros(num_sample) + np.inf\n",
        "        indT_H = int(np.ceil(self.T_H/self.dt))\n",
        "\n",
        "        for i in range(indT_H):\n",
        "            dw_sample, V_new = self.one_step(V[-1, :], mu, kappa)\n",
        "            V = np.vstack([V, V_new[None, :]]) # Stack vertically\n",
        "            dW = np.vstack([dW, dw_sample[None, :]]) # Adjusted for extra dimension\n",
        "\n",
        "        Lstart = V[-1, :] - self.x0*np.ones(num_sample)\n",
        "        i = indT_H+1  # Start from the index of T_H in the time grid\n",
        "        while np.any(T_A == np.inf) and i < 20/self.dt:\n",
        "            dw_sample, V_new = self.one_step(V[-1, :], mu, kappa)\n",
        "            V = np.vstack([V, V_new[None, :]])\n",
        "            dW = np.vstack([dW, dw_sample[None, :]])\n",
        "\n",
        "            L = V_new - self.x0\n",
        "            condition = L * Lstart <= 0\n",
        "            # Update T_A for indices where condition is met and T_A not changed yet\n",
        "            for j in np.where((condition) & (T_A == np.inf))[0]:\n",
        "                T_A[j] = i\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        if i == 20 * indT_H:\n",
        "            print(\"The maximum return time over num_sample is large, greater than T=20. Consider changing parameters of the stochastic factor.\")\n",
        "\n",
        "        return np.array(T_A, int), dW, V\n",
        "\n",
        "\n",
        "    def sol_SDE(self, X, dW, drift, vol, tinit, gammainit):\n",
        "        x_sample = np.zeros([len(X[:,0]), len(X[0,:])])\n",
        "        x_sample[tinit, :] = np.ones(len(X[0,:])) * gammainit\n",
        "        for i in range(tinit, len(X[:,0])-1):\n",
        "          x_sample[i+1, :] = x_sample[i, :] + x_sample[i, :]*drift(X[i, :])*self.dt \\\n",
        "            + x_sample[i,:] * tf.reduce_sum(vol(X[i, :]) * dW[i, :], axis=1)\n",
        "\n",
        "        return x_sample\n",
        "\n",
        "\n",
        "class OrnsteinUhlenbeck(StochasticFactor):\n",
        "    def __init__(self, x0, dt, T_H, dim, muval, nu, kappa):\n",
        "        super().__init__(x0, dt, T_H, dim)\n",
        "        self.muval = muval\n",
        "        self.kappa = kappa\n",
        "        self.x0 = x0\n",
        "        self.nu = nu\n",
        "\n",
        "    def mu(self, x):\n",
        "      return -self.muval * (x - self.nu)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eKoNEJ1kQ_j"
      },
      "source": [
        "# ErgodicFactorModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vW1c4ic7xSX5"
      },
      "outputs": [],
      "source": [
        "class ErgodicFactorModel():\n",
        "  def __init__(self, stochastic_factor):\n",
        "    self.stochastic_factor = stochastic_factor\n",
        "    self.dt = stochastic_factor.dt\n",
        "    self.T_H = stochastic_factor.T_H\n",
        "    self.dim = stochastic_factor.dim\n",
        "\n",
        "  def proj_Pi(self, Pi, X):\n",
        "      Pi = tf.convert_to_tensor(Pi, dtype=X.dtype)\n",
        "      # Ensure Pi has shape (dim, 2)\n",
        "      assert Pi.shape[1] == 2, \"Pi should have shape (dim, 2)\"\n",
        "\n",
        "      Pi_expanded = tf.expand_dims(tf.expand_dims(Pi, 0), 0)\n",
        "\n",
        "      X_expanded = tf.expand_dims(X, -1)\n",
        "      lower_mask = X_expanded < Pi_expanded[..., 0:1]\n",
        "      upper_mask = X_expanded > Pi_expanded[..., 1:2]\n",
        "\n",
        "      X_proj = tf.where(lower_mask, Pi_expanded[..., 0:1], X_expanded)\n",
        "      X_proj = tf.where(upper_mask, Pi_expanded[..., 1:2], X_proj)\n",
        "\n",
        "      X_proj = tf.squeeze(X_proj, -1)\n",
        "\n",
        "      return X_proj\n",
        "\n",
        "  # forward scheme eBSDE\n",
        "  def forward_BSDE(self, T_A, dW, V, ergodic_model, kerasModel):\n",
        "      nbr_traj = len(V[0,:])\n",
        "      maxind = max(T_A)\n",
        "      dW, V = tf.convert_to_tensor(dW, dtype=tf.float32), tf.convert_to_tensor(V, dtype=tf.float32)\n",
        "      Y_sim = np.zeros([maxind+1, nbr_traj])\n",
        "      Z_sim = np.zeros((maxind, nbr_traj, self.dim))\n",
        "\n",
        "      Y_trans = ergodic_model.Y0 * tf.ones([nbr_traj], dtype=tf.float32)\n",
        "      Y_sim[0, :] = Y_trans.numpy()\n",
        "      for iStep in range(maxind):\n",
        "          Z = kerasModel(tf.expand_dims(V[iStep, :nbr_traj], axis=-1))\n",
        "          Y_trans = Y_trans - self.dt * tf.transpose(ergodic_model.f(V[iStep, :nbr_traj], Z)) + kerasModel.lambd.numpy() * tf.ones([nbr_traj], dtype=tf.float32) * self.dt + tf.reduce_sum(Z * dW[iStep, :], axis=1)\n",
        "          Y_sim[iStep+1, :] = Y_trans.numpy()\n",
        "          Z_sim[iStep, :] = Z.numpy()\n",
        "\n",
        "      return Y_sim, Z_sim\n",
        "\n",
        "  def mean_abs_error(self, T_A, Y_ex, Y_sim):\n",
        "      indTH = int(np.ceil(self.T_H / self.dt))\n",
        "      nbr_traj = Y_ex.shape[1]\n",
        "      Y_ex_mod = Y_ex[:indTH+1, :nbr_traj]\n",
        "      mask_rel = tf.cast(tf.math.abs(Y_ex_mod) > 0, dtype=tf.float32)\n",
        "      Y_sim_mod = Y_sim[:indTH+1, :nbr_traj]\n",
        "      # Absolute error\n",
        "      Etot = np.abs(Y_ex_mod - Y_sim_mod)\n",
        "      # Relative error\n",
        "      Etotrel = np.where(Y_ex_mod != 0, np.abs((Y_ex_mod - Y_sim_mod) / Y_ex_mod), 0)\n",
        "\n",
        "      Emoy = np.sum(Etot, axis=1) / nbr_traj\n",
        "      Emoy_rel = np.sum(Etotrel, axis=1) / np.sum(mask_rel, axis=1)\n",
        "\n",
        "      return Emoy, Emoy_rel\n",
        "\n",
        "  def integral_error(self, T_A, Y_ex, Y_sim, Z_ex, Z_sim):\n",
        "      indTH = int(np.ceil(self.T_H / self.dt))\n",
        "      nbr_traj = Y_ex.shape[1]\n",
        "      interrY = tf.reduce_mean(self.dt * tf.reduce_sum(abs(Y_ex[:indTH, :nbr_traj] - Y_sim[:indTH, :nbr_traj]), axis=0))\n",
        "      interrZ = tf.reduce_mean(self.dt * tf.reduce_sum(tf.norm(Z_ex[:indTH, :nbr_traj, :] - Z_sim[:indTH, :nbr_traj, :], axis=-1)**2, axis=0))\n",
        "\n",
        "      return interrY, interrZ\n",
        "\n",
        "class EV(ErgodicFactorModel):\n",
        "  def __init__(self, stochastic_factor):\n",
        "      super().__init__(stochastic_factor)\n",
        "\n",
        "  # Approximation Monte Carlo lambda for generators depending only on the stochastic factor V\n",
        "  def lambdapprox(self, V, T_A):\n",
        "      T_A = tf.convert_to_tensor(T_A, dtype=tf.float32)\n",
        "      V = tf.convert_to_tensor(V, dtype=tf.float32)\n",
        "      T_Aint = tf.cast(T_A, dtype=tf.int32)\n",
        "\n",
        "      timestep_range = tf.range(tf.shape(V)[0])[:, tf.newaxis]\n",
        "      selec_index = timestep_range <= T_Aint\n",
        "\n",
        "      V_selec = tf.where(selec_index, V, tf.zeros_like(V))\n",
        "      f_values = self.f(V_selec, 0)\n",
        "      appint_values = tf.reduce_sum(f_values * self.stochastic_factor.dt, axis=0)\n",
        "\n",
        "      meanappint = tf.reduce_sum(appint_values)\n",
        "      meanT_R = tf.reduce_sum(T_A * self.dt)\n",
        "\n",
        "      return float(meanappint / meanT_R)\n",
        "\n",
        "# The solutions for Example 1 and Example 2 are valid for a stochastic factor of type OrnsteinUhlenbeck and in dimension 1.\n",
        "class Example1(EV):\n",
        "  def __init__(self, ornstein_uhlenbeck, Cv):\n",
        "      super().__init__(ornstein_uhlenbeck)\n",
        "      self.Cv = Cv\n",
        "      self.muval = ornstein_uhlenbeck.muval\n",
        "      self.kappa = ornstein_uhlenbeck.kappa\n",
        "      self.Y0 = (self.Cv/(self.muval + (1/2)*self.kappa[0]**2))*np.sqrt(2*np.pi)*scipy.stats.norm.cdf(ornstein_uhlenbeck.x0, 0, 1)\n",
        "      self.Zlim = tf.norm(tf.constant(self.kappa, dtype=tf.float32))*(self.Cv / (self.muval - self.Cv))\n",
        "      self.lambdlim = self.Cv*tf.exp(-1**2/2)\n",
        "      self.lambd_ex = 0\n",
        "\n",
        "  # Driver\n",
        "  def f(self, v, z):\n",
        "      return v*self.Cv*tf.exp(-v**2/2)\n",
        "\n",
        "  # Exact solution\n",
        "  def y_ex(self, V):\n",
        "      return (self.Cv/(self.muval + (1/2)*self.kappa[0]**2))*np.sqrt(2*np.pi)*scipy.stats.norm.cdf(V, 0, 1)\n",
        "\n",
        "  def z_ex(self, V):\n",
        "      return (self.Cv/(self.muval + (1/2)*self.kappa[0]**2))*tf.exp(-V**2/2)\n",
        "\n",
        "\n",
        "class Example2(EV):\n",
        "  def __init__(self, ornstein_uhlenbeck, Cv):\n",
        "      super().__init__(ornstein_uhlenbeck)\n",
        "      self.Cv = Cv\n",
        "      self.dt = ornstein_uhlenbeck.dt\n",
        "      self.muval = ornstein_uhlenbeck.muval\n",
        "      self.kappa = np.sqrt(2*self.muval) #kappa fixed from drift, see Annex A\n",
        "      self.x0 = ornstein_uhlenbeck.x0\n",
        "\n",
        "      self.A1 = self.Cv/(self.kappa**2)\n",
        "      self.A2 = 2*self.A1\n",
        "      self.Y0 = 1+quad(lambda y: np.exp(y**2/2)*(self.A1*np.exp(-y**2) + self.A2*(scipy.stats.norm.cdf(y, 0, 1) - 1)), 0, self.x0)[0]\n",
        "\n",
        "      self.Zlim = tf.norm(self.kappa)*(self.Cv / (self.muval - self.Cv))\n",
        "      self.lambd_ex = self.Cv / np.sqrt(2*np.pi)\n",
        "      self.lambdlim = self.Cv*tf.exp(-1**2/2)\n",
        "\n",
        "  def integrandpos(self, y):\n",
        "      return np.exp(y**2/2)*(self.A1*np.exp(-y**2) + self.A2*(scipy.stats.norm.cdf(y, 0, 1) - 1))\n",
        "\n",
        "  def integrandneg(self, y):\n",
        "      return np.exp(y**2/2)*(-self.A1*np.exp(-y**2) + self.A2*scipy.stats.norm.cdf(y, 0, 1))\n",
        "\n",
        "  # Exact solution\n",
        "  def y_ex(self, T_A, X):\n",
        "      nbr_time_step, nbr_traj = X.shape\n",
        "      Y_ex = np.zeros((nbr_time_step, nbr_traj))\n",
        "      for iStep in range(nbr_time_step):\n",
        "          listM = tf.cast(iStep <= T_A, tf.float32)\n",
        "          Neg = tf.cast(X[iStep, :] < 0, tf.float32)\n",
        "          Pos = tf.cast(X[iStep, :] >= 0, tf.float32)\n",
        "          Xpos = X[iStep, :] * Pos * listM\n",
        "          Xneg = X[iStep, :] * Neg * listM\n",
        "          for m in range(nbr_traj):\n",
        "              if listM[m]:  # Only integrate for active trajectories\n",
        "                  if Pos[m]:\n",
        "                      Y_ex[iStep, m] = 1+ quad(self.integrandpos, 0, Xpos[m])[0]\n",
        "                  if Neg[m]:\n",
        "                      Y_ex[iStep, m] = 1+quad(self.integrandneg, 0, Xneg[m])[0]\n",
        "\n",
        "      return Y_ex\n",
        "\n",
        "  def z_ex(self, X):\n",
        "    Pos = X >= 0\n",
        "    Neg = X < 0\n",
        "    return Neg*self.integrandneg(X) + Pos * self.integrandpos(X)\n",
        "\n",
        "  # Driver\n",
        "  def f(self, v, z):\n",
        "      return self.Cv*abs(v)*tf.exp(-v**2/2)\n",
        "\n",
        "\n",
        "class ErgodicPowerSE(ErgodicFactorModel):\n",
        "  def __init__(self, stochastic_factor, market_price, p, b, delt):\n",
        "      super().__init__(stochastic_factor)\n",
        "      self.delt = delt #risk aversion\n",
        "      self.gamma = 1 /(1 - self.delt)\n",
        "      self.Cv = (1/2)*(delt/(1-delt))*np.linalg.norm(p)*np.max([2, b])\n",
        "      self.lambdlim = (1/2)*(delt/(1-delt))*b**2\n",
        "      self.dt = stochastic_factor.dt\n",
        "      self.mu = stochastic_factor.mu\n",
        "      self.kappa = stochastic_factor.kappa\n",
        "      self.Zlim = tf.norm(tf.constant(self.kappa, dtype=tf.float32))*(self.Cv / (self.stochastic_factor.muval - self.Cv))\n",
        "      self.Y0 = 0.\n",
        "      self.market_price = market_price\n",
        "      self.lambd_ex = 'Not known'\n",
        "\n",
        "  def thet(self, x):\n",
        "    return self.market_price(x)\n",
        "\n",
        "  def beta(self, V, l):\n",
        "    return (self.delt / (1 - self.delt)) * (self.gamma / 2) * tf.norm(self.thet(V), axis=-1)**2 - self.gamma * l\n",
        "\n",
        "  def nu(self, V):\n",
        "    return (self.delt / (1 - self.delt)) * self.thet(V)\n",
        "\n",
        "  # Approximation Monte Carlo - Power utility example\n",
        "  def loss(self, T_A, X, dW, l):\n",
        "    Nbsimul = len(X[0, :])\n",
        "    G = self.stochastic_factor.sol_SDE(X, dW, lambda x: self.beta(x, l), self.nu, 0, 1)\n",
        "    selected_G = tf.gather_nd(G, tf.stack([T_A, tf.range(Nbsimul)], axis=1))\n",
        "    lossval = tf.abs(tf.reduce_mean(selected_G) - 1)\n",
        "\n",
        "    return lossval.numpy()\n",
        "\n",
        "  def lambda_app_newt(self, num_sample):\n",
        "    T_A, dW, V = self.stochastic_factor.sample(self.mu, self.kappa, num_sample)\n",
        "    return scipy.optimize.fsolve(lambda l: self.loss(T_A, V, dW, l), 0)\n",
        "\n",
        "  # Driver\n",
        "  def f(self, v, z):\n",
        "    return (1/2)*self.delt/(1 - self.delt)*tf.norm(z + self.thet(v), axis = -1)**2 + (1/2)*tf.norm(z, axis=-1)**2\n",
        "\n",
        "  # Exact solution\n",
        "  def yex(self, EC):\n",
        "    return self.Y0 + np.reshape((1/self.gamma)*np.log(EC),  (len(EC[:,0]), len(EC[0, :])))\n",
        "\n",
        "  # optimal portoflio\n",
        "  def optimal_strategy(self, Pi, V, kerasModel):\n",
        "    Ztot = np.zeros((len(V[:,0]), len(V[0,:]), len(Pi) ))\n",
        "    thetV = np.zeros((len(V[:,0]), len(V[0,:]), len(Pi) ))\n",
        "    for iStep in range(len(V[:,0])):\n",
        "      Z = kerasModel(tf.expand_dims(V[iStep, :len(V[0,:])], axis=-1))\n",
        "      Ztot[iStep, :] = Z.numpy()\n",
        "      thetV[iStep,:] = self.market_price(V[iStep,:])\n",
        "\n",
        "    return self.proj_Pi(Pi, (1/(1-self.delt))*(thetV + Ztot))\n",
        "\n",
        "\n",
        "class ErgodicPowerGen(ErgodicFactorModel):\n",
        "  def __init__(self, stochastic_factor, market_price, p, b, delt):\n",
        "      super().__init__(stochastic_factor)\n",
        "      self.delt = delt #risk aversion\n",
        "      self.gamma = 1 /(1 - self.delt)\n",
        "      self.Cv = (1/2)*(delt/(1-delt))*np.linalg.norm(p)*np.max([2, b])\n",
        "      self.lambdlim = (1/2)*(delt/(1-delt))*b**2\n",
        "      self.dt = stochastic_factor.dt\n",
        "      self.mu = stochastic_factor.mu\n",
        "      self.muval = self.stochastic_factor.muval\n",
        "      self.kappa = stochastic_factor.kappa\n",
        "      self.Zlim = tf.norm(tf.constant(self.kappa, dtype=tf.float32))*(self.Cv / (self.muval - self.Cv))\n",
        "      self.market_price = market_price\n",
        "      self.Y0 = 0\n",
        "      self.lambd_ex = 'Not known'\n",
        "\n",
        "  def thet(self, x):\n",
        "    return self.market_price(x)\n",
        "\n",
        "  # Driver\n",
        "  def f(self, v, z):\n",
        "    return (1/2)*self.delt/(1 - self.delt)*(z[:,0] + self.thet(v)[:,0])**2 + (1/2)*tf.norm(z, axis=-1)**2\n",
        "\n",
        "  # Optimal portoflio\n",
        "  def optimal_strategy(self, Pi, V, kerasModel):\n",
        "    Ztot = np.zeros((len(V[:,0]), len(V[0,:]), len(Pi) ))\n",
        "    thetV = np.zeros((len(V[:,0]), len(V[0,:]), len(Pi) ))\n",
        "    for iStep in range(len(V[:,0])):\n",
        "      Z = kerasModel(tf.expand_dims(V[iStep, :len(V[0,:])], axis=-1))\n",
        "      Ztot[iStep, :] = Z.numpy()\n",
        "      thetV[iStep,:] = self.market_price(V[iStep,:])\n",
        "\n",
        "    return self.proj_Pi(Pi, (1/(1-self.delt))*(thetV + Ztot))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyM-UGIP7PbP"
      },
      "source": [
        "# Solvers_eBSDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K5uRstOz7U_9"
      },
      "outputs": [],
      "source": [
        "class SolverBase:\n",
        "    # mathModel          Math model\n",
        "    # modelKeras         Keras model\n",
        "    # lRate              Learning rate\n",
        "    def __init__(self, ErgodicFactorModel, lRate):\n",
        "        self.ErgodicFactorModel = ErgodicFactorModel\n",
        "        self.StochasticFactor = ErgodicFactorModel.stochastic_factor\n",
        "        self.lRate = lRate\n",
        "\n",
        "    @tf.function\n",
        "    def proj(self, Z, Zlim):\n",
        "        norms = tf.norm(Z, axis=1, keepdims=True)\n",
        "        # Calculate scaling factors for rows where the norm exceeds Zlim\n",
        "        scaling_factors = tf.where(norms > Zlim, Zlim / norms, 1)\n",
        "        Z_proj = Z * scaling_factors\n",
        "\n",
        "        return Z_proj\n",
        "\n",
        "# Global solver\n",
        "class SolverGlobaleBSDE(SolverBase):\n",
        "    def __init__(self, ErgodicFactorModel, modelKerasUZ , lRate):\n",
        "        super().__init__(ErgodicFactorModel, lRate)\n",
        "        self.modelKerasUZ = modelKerasUZ\n",
        "\n",
        "    def train(self, batchSize, batchSizeVal, num_epoch, num_epochExt):\n",
        "        @tf.function\n",
        "        def optimizeBSDE(nbSimul):\n",
        "            T_A, dW, V = self.StochasticFactor.sample(self.StochasticFactor.mu, self.StochasticFactor.kappa, nbSimul)\n",
        "            dW, V = tf.convert_to_tensor(dW, dtype=tf.float32), tf.convert_to_tensor(V, dtype=tf.float32)\n",
        "            Y = []\n",
        "            maxind = max(T_A)\n",
        "            Y_trans = self.ErgodicFactorModel.Y0 * tf.ones([nbSimul], dtype=tf.float32)\n",
        "            for iStep in range(int(maxind)):\n",
        "              input_tensor = tf.expand_dims(V[iStep, :], axis=-1)\n",
        "              Z = self.modelKerasUZ(input_tensor)\n",
        "              Y_trans = Y_trans - self.StochasticFactor.dt * self.ErgodicFactorModel.f(V[iStep, :], Z) + self.modelKerasUZ.lambd * tf.ones([nbSimul], dtype=tf.float32) * self.StochasticFactor.dt + tf.reduce_sum(Z * dW[iStep, :], axis=1)\n",
        "              indices = tf.where(T_A == iStep)[:, 0]\n",
        "              Y_trans_selected = tf.gather(Y_trans, indices)\n",
        "              Y.append(Y_trans_selected)\n",
        "\n",
        "            Y = tf.concat(Y, axis=0)\n",
        "\n",
        "            return tf.reduce_mean(tf.square(Y - self.ErgodicFactorModel.Y0))\n",
        "\n",
        "        # train to optimize control\n",
        "        @tf.function\n",
        "        def trainOptNN(nbSimul, optimizer):\n",
        "            with tf.GradientTape() as tape:\n",
        "                objFunc_Z = optimizeBSDE(nbSimul)\n",
        "            gradients = tape.gradient(objFunc_Z, self.modelKerasUZ.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, self.modelKerasUZ.trainable_variables))\n",
        "            return objFunc_Z\n",
        "\n",
        "        optimizerN = optimizers.Adam(learning_rate = self.lRate)\n",
        "\n",
        "        self.listlambd = []\n",
        "        self.lossList = []\n",
        "        self.duration = 0\n",
        "        for iout in range(num_epochExt):\n",
        "            start_time = time.time()\n",
        "            for epoch in range(num_epoch):\n",
        "                # un pas de gradient stochastique\n",
        "                trainOptNN(batchSize, optimizerN)\n",
        "            end_time = time.time()\n",
        "            rtime = end_time-start_time\n",
        "            self.duration += rtime\n",
        "            objError_Yterm = optimizeBSDE(batchSizeVal)\n",
        "            lambd = self.modelKerasUZ.lambd.numpy()\n",
        "            print(\" Error\",objError_Yterm.numpy(),  \" elapsed time %5.3f s\" % self.duration, \"lambda so far \",lambd, 'epoch', iout)\n",
        "            self.listlambd.append(lambd)\n",
        "            self.lossList.append(objError_Yterm)\n",
        "\n",
        "        return self.listlambd, self.lossList\n",
        "\n",
        "# Local solver\n",
        "class SolverLocaleBSDE(SolverBase):\n",
        "    def __init__(self, ErgodicFactorModel, modelKerasY, modelKerasZ, lRate):\n",
        "        super().__init__(ErgodicFactorModel, lRate)\n",
        "        self.modelKerasY = modelKerasY\n",
        "        self.modelKerasZ = modelKerasZ\n",
        "\n",
        "    def train(self, batchSize, batchSizeVal, num_epoch, num_epochExt):\n",
        "        @tf.function\n",
        "        def optimizeBSDE(nbSimul):\n",
        "            T_A, dW, V = self.StochasticFactor.sample(self.StochasticFactor.mu, self.StochasticFactor.kappa, nbSimul)\n",
        "            dW, V = tf.convert_to_tensor(dW, dtype=tf.float32), tf.convert_to_tensor(V, dtype=tf.float32)\n",
        "            maxind = max(T_A)\n",
        "            Loss = []\n",
        "            loss_term = tf.zeros([nbSimul], dtype=tf.float32)\n",
        "            Y_trans = self.ErgodicFactorModel.Y0 * tf.ones([nbSimul], dtype=tf.float32)\n",
        "            for iStep in range(int(maxind)):\n",
        "              input_tensor = tf.expand_dims(V[iStep, :], axis=-1)\n",
        "              Z = self.modelKerasZ(input_tensor)\n",
        "              Y = self.modelKerasY(input_tensor)\n",
        "              toAdd = self.StochasticFactor.dt * self.ErgodicFactorModel.f(V[iStep, :], Z) - self.modelKerasZ.lambd * tf.ones([nbSimul], dtype=tf.float32) * self.StochasticFactor.dt - tf.reduce_sum(Z * dW[iStep, :], axis=1)\n",
        "              loss_term = loss_term + toAdd\n",
        "\n",
        "              indices = tf.where(T_A >= iStep)[:, 0]\n",
        "              loss_term_selec = tf.expand_dims(tf.gather(loss_term, indices), axis=-1)\n",
        "              Y_selec = tf.gather(Y, indices)\n",
        "\n",
        "              Loss.append(Y_selec  + loss_term_selec - self.ErgodicFactorModel.Y0*tf.ones_like(loss_term_selec))\n",
        "\n",
        "            Loss = tf.concat(Loss, axis=0)\n",
        "\n",
        "            return tf.reduce_mean(tf.square(Loss))\n",
        "\n",
        "        # train to optimize control\n",
        "        @tf.function\n",
        "        def trainOptNN(nbSimul, optimizerZ, optimizerY):\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                objFunc = optimizeBSDE(nbSimul)\n",
        "            gradientsZ = tape.gradient(objFunc, self.modelKerasZ.trainable_variables)\n",
        "            gradientsY = tape.gradient(objFunc, self.modelKerasY.trainable_variables)\n",
        "\n",
        "            optimizerZ.apply_gradients(zip(gradientsZ, self.modelKerasZ.trainable_variables))\n",
        "            optimizerY.apply_gradients(zip(gradientsY, self.modelKerasY.trainable_variables))\n",
        "\n",
        "            del tape\n",
        "            return objFunc\n",
        "\n",
        "        optimizerZ = optimizers.Adam(learning_rate=self.lRate)\n",
        "        optimizerY = optimizers.Adam(learning_rate=self.lRate)\n",
        "\n",
        "        self.listlambd = []\n",
        "        self.lossList = []\n",
        "        self.duration = 0\n",
        "        for iout in range(num_epochExt):\n",
        "            start_time = time.time()\n",
        "            for epoch in range(num_epoch):\n",
        "                # un pas de gradient stochastique\n",
        "                trainOptNN(batchSize, optimizerZ, optimizerY)\n",
        "            end_time = time.time()\n",
        "            rtime = end_time-start_time\n",
        "            self.duration += rtime\n",
        "            objError_Yterm = optimizeBSDE(batchSizeVal)\n",
        "            lambd = self.modelKerasZ.lambd.numpy()\n",
        "            print(\" Error\",objError_Yterm.numpy(),  \" elapsed time %5.3f s\" % self.duration, \"lambda so far \",lambd, 'epoch', iout)\n",
        "            self.listlambd.append(lambd)\n",
        "            self.lossList.append(objError_Yterm)\n",
        "\n",
        "\n",
        "        return self.listlambd, self.lossList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do2ULcmg-Tkb"
      },
      "source": [
        "# Main training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-NiECKc-TDR",
        "outputId": "fe5f3ab9-5a60-4fce-b113-fe8d5427f67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the example to run (Example1, Example2, ErgodicPowerSE, ErgodicPowerGen): Example1\n",
            "Enter the value for 'Cv': 1\n",
            "Enter a value for 'muval' greater than 1.0: 2\n",
            "'muval' set to: 2.0\n",
            "Enter the solver to run (Global, Local): Local\n",
            "lambda exact= 0\n",
            " Error 0.33045068  elapsed time 436.114 s lambda so far  [0.4640553] epoch 0\n",
            " Error 0.080516644  elapsed time 444.316 s lambda so far  [0.4488413] epoch 1\n",
            " Error 0.05699098  elapsed time 450.964 s lambda so far  [0.4416721] epoch 2\n",
            " Error 0.054124672  elapsed time 457.983 s lambda so far  [0.43461055] epoch 3\n",
            " Error 0.052743465  elapsed time 466.251 s lambda so far  [0.42659858] epoch 4\n",
            " Error 0.051084016  elapsed time 476.686 s lambda so far  [0.4177617] epoch 5\n",
            " Error 0.049055457  elapsed time 483.681 s lambda so far  [0.40820152] epoch 6\n",
            " Error 0.046804927  elapsed time 499.593 s lambda so far  [0.39800632] epoch 7\n",
            " Error 0.044441912  elapsed time 511.005 s lambda so far  [0.3872546] epoch 8\n",
            " Error 0.042025153  elapsed time 518.171 s lambda so far  [0.3760173] epoch 9\n",
            " Error 0.03958803  elapsed time 524.635 s lambda so far  [0.36436024] epoch 10\n",
            " Error 0.03715381  elapsed time 532.569 s lambda so far  [0.35234508] epoch 11\n",
            " Error 0.034741484  elapsed time 541.649 s lambda so far  [0.34002995] epoch 12\n",
            " Error 0.032367684  elapsed time 548.651 s lambda so far  [0.32747054] epoch 13\n",
            " Error 0.030046947  elapsed time 556.167 s lambda so far  [0.31471932] epoch 14\n",
            " Error 0.027792025  elapsed time 563.705 s lambda so far  [0.30182678] epoch 15\n",
            " Error 0.02561394  elapsed time 569.978 s lambda so far  [0.28884113] epoch 16\n",
            " Error 0.023521999  elapsed time 577.517 s lambda so far  [0.27580842] epoch 17\n",
            " Error 0.021523917  elapsed time 584.560 s lambda so far  [0.262773] epoch 18\n",
            " Error 0.01962588  elapsed time 590.879 s lambda so far  [0.24977733] epoch 19\n",
            " Error 0.017832581  elapsed time 598.397 s lambda so far  [0.23686202] epoch 20\n",
            " Error 0.016147306  elapsed time 604.658 s lambda so far  [0.2240658] epoch 21\n",
            " Error 0.014572123  elapsed time 611.362 s lambda so far  [0.2114262] epoch 22\n",
            " Error 0.013107823  elapsed time 618.851 s lambda so far  [0.1989788] epoch 23\n",
            " Error 0.011754098  elapsed time 625.154 s lambda so far  [0.18675771] epoch 24\n",
            " Error 0.010509624  elapsed time 633.600 s lambda so far  [0.17479542] epoch 25\n",
            " Error 0.009372135  elapsed time 641.094 s lambda so far  [0.1631229] epoch 26\n",
            " Error 0.008338537  elapsed time 647.449 s lambda so far  [0.15176949] epoch 27\n",
            " Error 0.0074049947  elapsed time 654.216 s lambda so far  [0.14076278] epoch 28\n",
            " Error 0.0065670186  elapsed time 661.764 s lambda so far  [0.13012855] epoch 29\n",
            " Error 0.005819588  elapsed time 668.049 s lambda so far  [0.11989071] epoch 30\n",
            " Error 0.0051572314  elapsed time 675.468 s lambda so far  [0.11007108] epoch 31\n",
            " Error 0.0045741447  elapsed time 683.002 s lambda so far  [0.10068928] epoch 32\n",
            " Error 0.004064272  elapsed time 689.302 s lambda so far  [0.09176256] epoch 33\n",
            " Error 0.0036214162  elapsed time 696.829 s lambda so far  [0.08330548] epoch 34\n",
            " Error 0.003239332  elapsed time 704.404 s lambda so far  [0.0753298] epoch 35\n",
            " Error 0.002911825  elapsed time 710.668 s lambda so far  [0.06784426] epoch 36\n",
            " Error 0.0026328287  elapsed time 718.229 s lambda so far  [0.06085428] epoch 37\n",
            " Error 0.0023964997  elapsed time 725.418 s lambda so far  [0.05436189] epoch 38\n",
            " Error 0.002197285  elapsed time 731.763 s lambda so far  [0.0483655] epoch 39\n",
            " Error 0.0020299866  elapsed time 739.294 s lambda so far  [0.04285977] epoch 40\n",
            " Error 0.0018898224  elapsed time 745.866 s lambda so far  [0.03783559] epoch 41\n",
            " Error 0.0017724652  elapsed time 752.305 s lambda so far  [0.03328012] epoch 42\n",
            " Error 0.0016740707  elapsed time 759.844 s lambda so far  [0.02917683] epoch 43\n",
            " Error 0.0015912973  elapsed time 766.184 s lambda so far  [0.02550575] epoch 44\n",
            " Error 0.0015213136  elapsed time 773.664 s lambda so far  [0.02224374] epoch 45\n",
            " Error 0.0014617771  elapsed time 781.262 s lambda so far  [0.01936493] epoch 46\n",
            " Error 0.0014108146  elapsed time 787.568 s lambda so far  [0.01684119] epoch 47\n",
            " Error 0.0013669708  elapsed time 798.698 s lambda so far  [0.01464275] epoch 48\n",
            " Error 0.0013291495  elapsed time 806.142 s lambda so far  [0.01273889] epoch 49\n",
            " Error 0.0012965329  elapsed time 818.355 s lambda so far  [0.01109866] epoch 50\n",
            " Error 0.0012684965  elapsed time 829.975 s lambda so far  [0.00969167] epoch 51\n",
            " Error 0.0012445254  elapsed time 839.085 s lambda so far  [0.0084888] epoch 52\n",
            " Error 0.0012241469  elapsed time 846.057 s lambda so far  [0.00746283] epoch 53\n",
            " Error 0.001206891  elapsed time 852.426 s lambda so far  [0.00658894] epoch 54\n",
            " Error 0.0011922828  elapsed time 859.991 s lambda so far  [0.00584497] epoch 55\n",
            " Error 0.0011798558  elapsed time 866.352 s lambda so far  [0.00521148] epoch 56\n",
            " Error 0.0011691822  elapsed time 872.920 s lambda so far  [0.00467157] epoch 57\n",
            " Error 0.0011598922  elapsed time 880.481 s lambda so far  [0.00421067] epoch 58\n",
            " Error 0.0011516899  elapsed time 886.770 s lambda so far  [0.00381614] epoch 59\n",
            " Error 0.0011443488  elapsed time 894.080 s lambda so far  [0.00347699] epoch 60\n",
            " Error 0.0011377045  elapsed time 901.620 s lambda so far  [0.00318365] epoch 61\n",
            " Error 0.0011316388  elapsed time 907.934 s lambda so far  [0.00292775] epoch 62\n",
            " Error 0.0011260675  elapsed time 915.460 s lambda so far  [0.00270211] epoch 63\n",
            " Error 0.00112093  elapsed time 922.943 s lambda so far  [0.00250056] epoch 64\n",
            " Error 0.0011161815  elapsed time 929.251 s lambda so far  [0.00231801] epoch 65\n",
            " Error 0.0011117872  elapsed time 936.788 s lambda so far  [0.00215031] epoch 66\n",
            " Error 0.0011077202  elapsed time 944.366 s lambda so far  [0.00199423] epoch 67\n",
            " Error 0.0011039589  elapsed time 950.651 s lambda so far  [0.00184734] epoch 68\n",
            " Error 0.0011004838  elapsed time 958.180 s lambda so far  [0.00170788] epoch 69\n",
            " Error 0.001097279  elapsed time 965.211 s lambda so far  [0.00157463] epoch 70\n",
            " Error 0.0010943306  elapsed time 971.497 s lambda so far  [0.00144684] epoch 71\n",
            " Error 0.0010916237  elapsed time 979.030 s lambda so far  [0.00132402] epoch 72\n",
            " Error 0.0010891461  elapsed time 985.335 s lambda so far  [0.00120592] epoch 73\n",
            " Error 0.0010868848  elapsed time 992.100 s lambda so far  [0.00109244] epoch 74\n",
            " Error 0.001084827  elapsed time 999.668 s lambda so far  [0.00098355] epoch 75\n",
            " Error 0.0010829604  elapsed time 1006.411 s lambda so far  [0.00087927] epoch 76\n",
            " Error 0.0010812725  elapsed time 1013.811 s lambda so far  [0.00077962] epoch 77\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "T_H = 1\n",
        "h = 0.01\n",
        "kappa = [0.8]\n",
        "dim = len(kappa)\n",
        "nu = 0\n",
        "x0 = 0\n",
        "Mlambd = 100000\n",
        "\n",
        "# Power utility examples parameters\n",
        "delt = 0.5  # risk aversion\n",
        "p = [0.8]  # Lipschitz constant market price of risk - of dimension dim\n",
        "b = 3  # born market price of risk\n",
        "Pi = [[-np.inf, np.inf], [0, 0]]\n",
        "\n",
        "def market_price(v):\n",
        "    v = tf.convert_to_tensor(v, dtype=tf.float32)\n",
        "    pa = tf.convert_to_tensor(p, dtype=tf.float32)\n",
        "    v = tf.reshape(v, (-1, 1))\n",
        "    pv = pa * v\n",
        "\n",
        "    norm_pv = tf.norm(pv, axis=1, keepdims=True)\n",
        "    condition = norm_pv > b\n",
        "    pv = tf.where(condition, (pv / norm_pv) * b, pv)\n",
        "\n",
        "    return pv\n",
        "\n",
        "# NN Parameters\n",
        "nbNeuron = 20 + dim\n",
        "nbLayer = 2\n",
        "num_epochExt = 100\n",
        "num_epoch = 100\n",
        "batchSize = 64\n",
        "lRate = 0.0003\n",
        "activation = tf.nn.tanh\n",
        "\n",
        "# Select example\n",
        "example_name = input(\"Enter the example to run (Example1, Example2, ErgodicPowerSE, ErgodicPowerGen): \")\n",
        "if example_name in ['Example1', 'Example2']:\n",
        "    Cv = float(input(\"Enter the value for 'Cv': \"))\n",
        "else:\n",
        "    Cv = (1 / 2) * (delt / (1 - delt)) * np.linalg.norm(p) * np.max([2, b])\n",
        "    print('Cv =', Cv)\n",
        "\n",
        "# Prompt for a valid value of muval\n",
        "muval = None\n",
        "while muval is None or muval <= Cv:\n",
        "    try:\n",
        "        muval_input = input(f\"Enter a value for 'muval' greater than {Cv}: \")\n",
        "        muval = float(muval_input)\n",
        "        if muval <= Cv:\n",
        "            print(f\"The value of 'muval' must be greater than {Cv}. You entered: {muval}\")\n",
        "    except ValueError:\n",
        "        print(\"Please enter a valid numeric value.\")\n",
        "\n",
        "print(f\"'muval' set to: {muval}\")\n",
        "\n",
        "# Initialize the selected example\n",
        "if example_name == 'Example2':\n",
        "    kappa = [np.sqrt(2*muval)]\n",
        "\n",
        "solver_name = input(\"Enter the solver to run (Global, Local): \")\n",
        "\n",
        "stochastic_factor = OrnsteinUhlenbeck(x0, h, T_H, dim, muval, nu, kappa)\n",
        "if example_name == 'Example1':\n",
        "    example = Example1(stochastic_factor, Cv)\n",
        "elif example_name == 'Example2':\n",
        "    kappa = [np.sqrt(2*muval)]\n",
        "    example = Example2(stochastic_factor, Cv)\n",
        "elif example_name == 'ErgodicPowerSE':\n",
        "    stochastic_factor = OrnsteinUhlenbeck(x0, h, T_H, dim, muval, nu, kappa)\n",
        "    example = ErgodicPowerSE(stochastic_factor, market_price, p, b, delt)\n",
        "elif example_name == 'ErgodicPowerGen':\n",
        "    stochastic_factor = OrnsteinUhlenbeck(x0, h, T_H, dim, muval, nu, kappa)\n",
        "    example = ErgodicPowerGen(stochastic_factor, market_price, p, b, delt)\n",
        "else:\n",
        "    raise ValueError(\"Invalid example name.\")\n",
        "\n",
        "# Some values\n",
        "lambda_lim = example.lambdlim\n",
        "if hasattr(example, 'lambd_ex'):\n",
        "    print(f'lambda exact= {example.lambd_ex}')\n",
        "\n",
        "# Neural network\n",
        "layerSize = nbNeuron * np.ones((nbLayer,), dtype=np.int32)\n",
        "if solver_name == 'Global':\n",
        "  kerasModel = Net(True, lambda_lim, dim, layerSize, activation)\n",
        "  solver = SolverGlobaleBSDE(example, kerasModel, lRate)\n",
        "if solver_name == 'Local':\n",
        "  kerasModelY = Net(False, lambda_lim, 1, layerSize, activation)\n",
        "  kerasModelZ = Net(True, lambda_lim, dim, layerSize, activation)\n",
        "  solver = SolverLocaleBSDE(example, kerasModelY, kerasModelZ, lRate)\n",
        "\n",
        "# train and  get solution\n",
        "lambdlist, lossT_Hlist = solver.train(batchSize, batchSize * 100, num_epoch, num_epochExt)\n",
        "\n",
        "###### PLOTS ######\n",
        "if example_name =='Example1' or example_name == 'Example2':\n",
        "  lambda_benchmark = example.lambd_ex\n",
        "  lambd_label = r'$\\lambda$ exact'\n",
        "\n",
        "if example_name == 'ErgodicPowerSE':\n",
        "  lambd_newt_values = []\n",
        "  for i in range(3):\n",
        "      lambd_newt = example.lambda_app_newt(Mlambd)\n",
        "      lambd_newt_values.append(lambd_newt)\n",
        "\n",
        "  std_lambd_newt = np.sqrt(np.var(lambd_newt_values))\n",
        "  lambda_benchmark = np.mean(lambd_newt_values)\n",
        "  lambd_label = r'$\\hat{\\lambda}$'\n",
        "  print('Mean lambda MC method =', lambda_benchmark)\n",
        "  print('Standard deviation lambda MC method =', std_lambd_newt)\n",
        "\n",
        "if example_name == 'ErgodicPowerGen':\n",
        "  lambda_benchmark = 'Not known'\n",
        "\n",
        "if solver_name == 'Global':\n",
        "  loss_name = r'$L^{B_{\\epsilon}}(\\theta, \\bar{\\lambda})$'\n",
        "  solver_label = 'GeBSDE'\n",
        "\n",
        "if solver_name == 'Local':\n",
        "  loss_name = r'$L_{loc}^{B_{\\epsilon}}(\\theta_{1}, \\theta_{2}, \\bar{\\lambda})$'\n",
        "  solver_label = 'LAeBSDE'\n",
        "\n",
        "Nepoch = range(0, num_epoch*num_epochExt, num_epoch)\n",
        "plt.figure()\n",
        "plt.plot(Nepoch, lossT_Hlist, label=f\"{loss_name} - {solver_label}\")\n",
        "plt.grid(True, which = 'both', linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.tight_layout()\n",
        "plt.title('Loss training result')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Nepoch, lambdlist, label=rf'$\\bar{{\\lambda}}$ - {solver_label}')\n",
        "if lambda_benchmark != 'Not known':\n",
        "    plt.plot(Nepoch, lambda_benchmark * np.ones(len(Nepoch)), \"r--\", label=lambd_label)\n",
        "plt.grid(True, which = 'both', linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.tight_layout()\n",
        "plt.title('Convergence lambda training result')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}